---
title: "mlR - Predict"
author: "Miguel Conde"
date: "3 de marzo de 2017"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.align = "center")
```

```{r}
require(mlr)
```

## Predicting Outcomes for New Data
There are two ways to pass the data:

Either pass the Task via the task argument or
pass a data frame via the newdata argument.
The first way is preferable if you want predictions for data already included in a Task.

In the following example we fit a gradient boosting machine to every second observation of the BostonHousing data set and make predictions on the remaining data in bh.task.

```{r}
n = getTaskSize(bh.task)
train.set = seq(1, n, by = 2)
test.set = seq(2, n, by = 2)
lrn = makeLearner("regr.gbm", n.trees = 100)
mod = train(lrn, bh.task, subset = train.set)

task.pred = predict(mod, task = bh.task, subset = test.set)
task.pred
```

The second way is useful if you want to predict data not included in the Task.

Here we cluster the iris data set without the target variable. All observations with an odd index are included in the Task and used for training. Predictions are made for the remaining observations.

```{r}
n = nrow(iris)
iris.train = iris[seq(1, n, by = 2), -5]
iris.test = iris[seq(2, n, by = 2), -5]
task = makeClusterTask(data = iris.train)
mod = train("cluster.kmeans", task)

newdata.pred = predict(mod, newdata = iris.test)
newdata.pred
```

## Accessing the prediction
Function predict returns a named list of class Prediction. Its most important element is $data which is a data frame that contains columns with the true values of the target variable (in case of supervised learning problems) and the predictions. Use as.data.frame for direct access.
```{r}
## Result of predict with data passed via task argument
head(as.data.frame(task.pred))
```

```{r}
## Result of predict with data passed via newdata argument
head(as.data.frame(newdata.pred))
```
As you can see when predicting from a Task, the resulting data frame contains an additional column, called id, which tells us which element in the original data set the prediction corresponds to.

A direct way to access the true and predicted values of the target variable(s) is provided by functions getPredictionTruth and getPredictionResponse.

```{r}
head(getPredictionTruth(task.pred))
```

```{r}
head(getPredictionResponse(task.pred))
```

## Extract Probabilities
Here is another cluster analysis example. We use fuzzy c-means clustering on the mtcars data set.
```{r}
lrn = makeLearner("cluster.cmeans", predict.type = "prob")
mod = train(lrn, mtcars.task)

pred = predict(mod, task = mtcars.task)
head(getPredictionProbabilities(pred))
```
For classification problems there are some more things worth mentioning. By default, class labels are predicted.
```{r}
## Linear discriminant analysis on the iris data set
mod = train("classif.lda", task = iris.task)

pred = predict(mod, task = iris.task)
pred
```

A confusion matrix can be obtained by calling getConfMatrix.
```{r}
getConfMatrix(pred)
```

In order to get predicted posterior probabilities we have to create a Learner with the appropriate predict.type.
```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, iris.task)

pred = predict(mod, newdata = iris)
head(as.data.frame(pred))
```

In addition to the probabilities, class labels are predicted by choosing the class with the maximum probability and breaking ties at random.

As mentioned above, the predicted posterior probabilities can be accessed via the getPredictionProbabilities function.

```{r}
head(getPredictionProbabilities(pred))
```

## Adjusting the threshold

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, task = sonar.task)

## Label of the positive class
getTaskDescription(sonar.task)$positive
```

```{r}
## Default threshold
pred1 = predict(mod, sonar.task)
pred1$threshold
```

```{r}
## Set the threshold value for the positive class
pred2 = setThreshold(pred1, 0.9)
pred2$threshold
```

```{r}
pred2
```

```{r}
## We can also set the effect in the confusion matrix
calculateConfusionMatrix(pred1)
```

```{r}
calculateConfusionMatrix(pred2)
```

In the binary case `getPredictionProbabilities` by default extracts the posterior probabilities of the positive class only.
```{r}
head(getPredictionProbabilities(pred1))
```

```{r}
head(getPredictionProbabilities(pred1, cl = c("M", "R")))
```

It works similarly for multiclass classification. The threshold has to be given by a named vector specifying the values by which each probability will be divided. The class with the maximum resulting value is then selected.

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, iris.task)
pred = predict(mod, newdata = iris)
pred$threshold
```

```{r}
table(as.data.frame(pred)$response)
```

```{r}
pred = setThreshold(pred, c(setosa = 0.01, versicolor = 50, virginica = 1))
pred$threshold
```

```{r}
table(as.data.frame(pred)$response)
```

## Visualizing the prediction
The function `plotLearnerPrediction` allows to visualize predictions, e.g., for teaching purposes or exploring models. It trains the chosen learning method for 1 or 2 selected features and then displays the predictions with ggplot.

For *classification*:

* We get a scatter plot of 2 features (by default the first 2 in the data set). 
* The type of symbol shows the true class labels of the data points. 
* Symbols with white border indicate misclassified observations. 
* The posterior probabilities (if the learner under consideration supports this) are represented by the background color where higher saturation means larger probabilities.

The plot title displays the ID of the Learner (in the following example CART), its parameters, its training performance and its cross-validation performance. mmce stands for mean misclassification error, i.e., the error rate. See the sections on performance and resampling for further explanations.

```{r}
lrn = makeLearner("classif.rpart", id = "CART")
plotLearnerPrediction(lrn, task = iris.task)
```

For *clustering*:

* We also get a scatter plot of two selected features. 
* The color of the points indicates the predicted cluster.

```{r}
lrn = makeLearner("cluster.kmeans")
plotLearnerPrediction(lrn, task = mtcars.task, 
                      features = c("disp", "drat"), cv = 0)
```

For *regression*, there are two types of plots:

* The 1D plot shows the target values in relation to a single feature, the regression curve and, if the chosen learner supports this, the estimated standard error.

```{r}
plotLearnerPrediction("regr.lm", features = "lstat", task = bh.task)
```

* The 2D variant, as in the classification case, generates a scatter plot of 2 features.  
    + The fill color of the dots illustrates the value of the target variable "medv", the background colors show the estimated mean.  
    + The plot does not represent the estimated standard error.  
  
```{r}
plotLearnerPrediction("regr.lm", features = c("lstat", "rm"), task = bh.task)
```

