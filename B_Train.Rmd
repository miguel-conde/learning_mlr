---
title: "mlR - Train"
author: "Miguel Conde"
date: "3 de marzo de 2017"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.align = "center")
```

```{r}
require(mlr)
```

## Training a Learner
We start with a classification example and perform a linear discriminant analysis on the iris data set.
```{r}
## Generate the task
task = makeClassifTask(data = iris, target = "Species")

## Generate the learner
lrn = makeLearner("classif.lda")

## Train the learner
mod = train(lrn, task)
mod
```

In the above example creating the Learner explicitly is not absolutely necessary. As a general rule, you have to generate the Learner yourself if you want to change any defaults, e.g., setting hyperparameter values or altering the predict type. Otherwise, train and many other functions also accept the class name of the learner and call makeLearner internally with default settings.

```{r}
mod = train("classif.lda", task)
mod
```

Below is a survival analysis example where a Cox proportional hazards model is fitted to the lung data set. Note that we use the corresponding lung.task provided by mlr.
```{r}
mod = train("surv.coxph", lung.task)
mod
```

## Accessing learner models
The fitted model in slot $learner.model of the WrappedModel object can be accessed using function getLearnerModel.

In the following example we cluster the Ruspini data set (which has four groups and two features) by KK-means with K=4 and extract the output of the underlying kmeans function.

```{r}
data(ruspini, package = "cluster")
plot(y ~ x, ruspini)
```

```{r}
## Generate the task
ruspini.task = makeClusterTask(data = ruspini)

## Generate the learner
lrn = makeLearner("cluster.kmeans", centers = 4)

## Train the learner
mod = train(lrn, ruspini.task)
mod
```

```{r}
## Peak into mod
names(mod)
```

```{r}
mod$learner
```

```{r}
mod$features
```

```{r}
mod$time
```

```{r}
## Extract the fitted model
getLearnerModel(mod)
```

## Further options and comments

By default, the whole data set in the Task is used for training. The subset argument of train takes a logical or integer vector that indicates which observations to use, for example if you want to split your data into a training and a test set or if you want to fit separate models to different subgroups in the data.

Below we fit a linear regression model to the BostonHousing data set (bh.task) and randomly select 1/3 of the data set for training.

```{r}
## Get the number of observations
n = getTaskSize(bh.task)

## Use 1/3 of the observations for training
train.set = sample(n, size = n/3)

## Train the learner
mod = train("regr.lm", bh.task, subset = train.set)
mod
```


if the learner supports this, you can specify observation weights that reflect the relevance of observations in the training process. Weights can be useful in many regards, for example to express the reliability of the training observations, reduce the influence of outliers or, if the data were collected over a longer time period, increase the influence of recent data. In supervised classification weights can be used to incorporate misclassification costs or account for class imbalance.

For example in the BreastCancer data set class benign is almost twice as frequent as class malignant. In order to grant both classes equal importance in training the classifier we can weight the examples according to the inverse class frequencies in the data set as shown in the following R code.

```{r}
## Calculate the observation weights
target = getTaskTargets(bc.task)
tab = as.numeric(table(target))
w = 1/tab[target]

train("classif.rpart", task = bc.task, weights = w)
```

As you may recall, it is also possible to set observation weights when creating the Task. As a general rule, you should specify them in make*Task if the weights really "belong" to the task and always should be used. Otherwise, pass them to train. The weights in train take precedence over the weights in Task.