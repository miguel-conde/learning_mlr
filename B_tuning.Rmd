---
title: "mlR - Tuning Hyperparameters"
author: "Miguel Conde"
date: "8 de marzo de 2017"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.align = "center")
```

```{r}
require(mlr)
```

## Basics
In order to tune a machine learning algorithm, you have to specify:

* the **search space**
* the **optimization algorithm** (aka **tuning method**)
* an **evaluation method**, i.e., a *resampling strategy* and a *performance measure*

An example of the **search space** could be searching values of the C parameter for SVM:

```{r}
## ex: create a search space for the C hyperparameter from 0.01 to 0.1
ps = makeParamSet(
  makeNumericParam("C", lower = 0.01, upper = 0.1)
)
```

An example of the **optimization algorithm** could be performing *random search* on the space:
```{r}
## ex: random search with 100 iterations
ctrl = makeTuneControlRandom(maxit = 100L)
```

An example of an **evaluation method** could be *3-fold CV* using *accuracy* as the performance measure:
```{r}
rdesc = makeResampleDesc("CV", iters = 3L)
measure = acc
```

## Specifying the search space
We first must define a space to search when tuning our learner. For example, maybe we want to tune several specific values of a hyperparameter or perhaps we want to define a space from 10−1010−10 to 10101010 and let the optimization algorithm decide which points to choose.

For example, we could define a search space with just the values 0.5, 1.0, 1.5, 2.0 for both C and gamma. Notice how we name each parameter as it's defined in the kernlab package:
```{r}
discrete_ps = makeParamSet(
  makeDiscreteParam("C", values = c(0.5, 1.0, 1.5, 2.0)),
  makeDiscreteParam("sigma", values = c(0.5, 1.0, 1.5, 2.0))
)
print(discrete_ps)
```

We could also define a continuous search space (using makeNumericParam instead of makeDiscreteParam) from $10^{-10}$ to $10^{10}$ for both parameters through the use of the trafo argument (trafo is short for transformation). 

Transformations work like this: All optimizers basically see the parameters on their original scale (from $10^{-10}$ to $10^{10}$ in this case) and produce values on this scale during the search. Right before they are passed to the learning algorithm, the transformation function is applied.

```{r}
num_ps = makeParamSet(
  makeNumericParam("C", lower = -10, upper = 10, trafo = function(x) 10^x),
  makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 10^x)
)
```

In order to standardize your workflow across several packages, whenever parameters in the underlying R functions should be passed in a list structure, mlr tries to give you direct access to each parameter and get rid of the list structure!

This is the case with the kpar argument of ksvm which is a list of kernel parameters like sigma. This allows us to interface with learners from different packages in the same way when defining parameters to tune!

## Specifying the optimization algorithm
Now that we have specified the search space, we need to choose an optimization algorithm for our parameters to pass to the ksvm learner. Optimization algorithms are considered TuneControl objects in mlr.

A grid search is one of the standard -- albeit slow -- ways to choose an appropriate set of parameters from a given search space.

In the case of discrete_ps above, since we have manually specified the values, grid search will simply be the cross product. We create the grid search object using the defaults, noting that we will have 4×4=164×4=16 combinations in the case of discrete_ps:

```{r}
ctrl = makeTuneControlGrid()
```

In the case of num_ps above, since we have only specified the upper and lower bounds for the search space, grid search will create a grid using equally-sized steps. By default, grid search will span the space in 10 equal-sized steps. The number of steps can be changed with the resolution argument. Here we change to 15 equal-sized steps in the space defined within the ParamSet object. For num_ps, this means 15 steps in the form of 10 ^ seq(-10, 10, length.out = 15):

```{r}
ctrl = makeTuneControlGrid(resolution = 15L)
```

Many other types of optimization algorithms are available. Check out [TuneControl](http://www.rdocumentation.org/packages/mlr/functions/TuneControl.html) for some examples.

The following tuners are available:

* makeTuneControlGrid
Grid search. All kinds of parameter types can be handled. You can either use their correct param type and resolution, or discretize them yourself by always using makeDiscreteParam in the par.set passed to tuneParams.

* makeTuneControlRandom
Random search. All kinds of parameter types can be handled.

* makeTuneControlDesign
Completely pre-specifiy a data.frame of design points to be evaluated during tuning. All kinds of parameter types can be handled.

* makeTuneControlCMAES
CMA (Covariance Matrix Adapting) Evolution Strategy with method cma_es. Can handle numeric(vector) and integer(vector) hyperparameters, but no dependencies. For integers the internally proposed numeric values are automatically rounded. The sigma variance parameter is initialized to 1/4 of the span of box-constraints per parameter dimension.

* makeTuneControlGenSA
Generalized simulated annealing with method GenSA. Can handle numeric(vector) and integer(vector) hyperparameters, but no dependencies. For integers the internally proposed numeric values are automatically rounded.

* makeTuneControlIrace
Tuning with iterated F-Racing with method irace. All kinds of parameter types can be handled. We return the best of the final elite candidates found by irace in the last race. Its estimated performance is the mean of all evaluations ever done for that candidate. More information on irace can be found in the TR at http://iridia.ulb.ac.be/IridiaTrSeries/link/IridiaTr2011-004.pdf.


Since grid search is normally too slow in practice, we'll also examine random search. In the case of discrete_ps, random search will randomly choose from the specified values. The maxit argument controls the amount of iterations.

```{r}
ctrl = makeTuneControlRandom(maxit = 10L)
```

In the case of num_ps, random search will randomly choose points within the space according to the specified bounds. Perhaps in this case we would want to increase the amount of iterations to ensure we adequately cover the space:

```{r}
ctrl = makeTuneControlRandom(maxit = 200L)
```

## Performing the tuning
Now that we have specified a search space and the optimization algorithm, it's time to perform the tuning. We will need to define a resampling strategy and make note of our performance measure.

We will use 3-fold cross-validation to assess the quality of a specific parameter setting. For this we need to create a resampling description:
```{r}
rdesc = makeResampleDesc("CV", iters = 3L)
```

Finally, by combining all the previous pieces, we can tune the SVM parameters by calling tuneParams. We will use discrete_ps with grid search:

```{r message=TRUE}
## Search space
discrete_ps = makeParamSet(
  makeDiscreteParam("C", values = c(0.5, 1.0, 1.5, 2.0)),
  makeDiscreteParam("sigma", values = c(0.5, 1.0, 1.5, 2.0))
)

## Optimization algorithm
ctrl = makeTuneControlGrid()

## Evaluation method
# Resampling Strategy: 3-fold CV
# Performance measure: mmce, default for task
#                      (As no performance measure was specified, by default
#                       the error rate (mmce) is used)
rdesc = makeResampleDesc("CV", iters = 3L)

## Perform the tuning
res = tuneParams("classif.ksvm", task = iris.task,
                 par.set = discrete_ps, 
                 control = ctrl, 
                 resampling = rdesc)
```

```{r}
res
```

Note that each measure "knows" if it is minimized or maximized during tuning.

```{r}
## error rate
mmce$minimize
```

```{r}
## accuracy
acc$minimize
```


Of course, you can pass other measures and also a list of measures to tuneParams. In the latter case the first measure is optimized during tuning, the others are simply evaluated. If you are interested in optimizing several measures simultaneously have a look at Advanced Tuning.

In the example below:

* We calculate the accuracy (acc) instead of the error rate. 
* We use function setAggregation, as described on the resampling page, to additionally obtain the standard deviation of the accuracy. 
* We also use random search with 100 iterations on the num_set we defined above 
* And set show.info to FALSE to hide the output for all 100 iterations:
```{r}
## Search space
num_ps = makeParamSet(
  makeNumericParam("C", lower = -10, upper = 10, trafo = function(x) 10^x),
  makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 10^x)
)

## Optimization algorithm
ctrl = makeTuneControlRandom(maxit = 100L)

## Evaluation method
# Resampling Strategy: 3-fold CV
# Performance measure: mmce, default for task
#                      (As no performance measure was specified, by default
#                       the error rate (mmce) is used)
rdesc = makeResampleDesc("CV", iters = 3L)

## Perform the tuning
res = tuneParams("classif.ksvm", task = iris.task, 
                 par.set = num_ps,
                 control = ctrl, 
                 resampling = rdesc, 
                 measures = list(acc, setAggregation(acc, test.sd)), 
                 show.info = FALSE)
res
```

Accessing the tuning result
The result object TuneResult allows you to access the best found settings `$x` and their estimated performance `$y`.
```{r}
res$x
```

```{r}
res$y
```
We can generate a Learner with optimal hyperparameter settings as follows:
```{r}
lrn = setHyperPars(makeLearner("classif.ksvm"), par.vals = res$x)
lrn
```

Then you can proceed as usual. Here we refit and predict the learner on the complete iris data set:
```{r}
m = train(lrn, iris.task)
predict(m, task = iris.task)
```

But what if you wanted to inspect the other points on the search path, not just the optimal?

## Investigating hyperparameter tuning effects
We can inspect all points evaluated during the search by using generateHyperParsEffectData:
```{r}
generateHyperParsEffectData(res)
```

Note that the result of generateHyperParsEffectData contains the parameter values on the original scale. In order to get the transformed parameter values instead, use the trafo argument:

```{r}
generateHyperParsEffectData(res, trafo = TRUE)
```


we can also generate performance on the train data along with the validation/test data:
```{r}
rdesc2 = makeResampleDesc("Holdout", predict = "both")

res2 = tuneParams("classif.ksvm", task = iris.task, resampling = rdesc2,
                  par.set = num_ps,
                  control = ctrl, 
                  measures = list(acc, setAggregation(acc, train.mean)),
                  show.info = FALSE)

generateHyperParsEffectData(res2)
```
We can also easily visualize the points evaluated by using plotHyperParsEffect. In the example below, we plot the performance over iterations, using the res from the previous section but instead with 2 performance measures:

```{r}
res = tuneParams("classif.ksvm", task = iris.task, 
                 resampling = rdesc, 
                 par.set = num_ps,
                 control = ctrl, measures = list(acc, mmce),
                 show.info = FALSE)

data = generateHyperParsEffectData(res)

plotHyperParsEffect(data, x = "iteration", y = "acc.test.mean",
                    plot.type = "line")
```

Note that by default, we only plot the current global optima. This can be changed with the global.only argument.

## Further comments
* Tuning works for all other tasks like regression, survival analysis and so on in a completely similar fashion.

* In longer running tuning experiments it is very annoying if the computation stops due to numerical or other errors. Have a look at on.learner.error in configureMlr as well as the examples given in section Configure mlr of this tutorial. You might also want to inform yourself about impute.val in TuneControl.

* As we continually optimize over the same data during tuning, the estimated performance value might be optimistically biased. A clean approach to ensure unbiased performance estimation is nested resampling, where we embed the whole model selection process into an outer resampling loop.