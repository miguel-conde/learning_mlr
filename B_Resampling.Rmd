---
title: "mlR - Resampling"
author: "Miguel Conde"
date: "7 de marzo de 2017"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.align = "center")
```

```{r}
require(mlr)
```

## Defining the resampling strategy
In mlr the resampling strategy can be defined via function makeResampleDesc. It requires a string that specifies the resampling method and, depending on the selected strategy, further information like the number of iterations. The supported resampling strategies are:

* Cross-validation ("CV"),
* Leave-one-out cross-validation ("LOO"),
* Repeated cross-validation ("RepCV"),
* Out-of-bag bootstrap and other variants like b632 ("Bootstrap"),
* Subsampling, also called Monte-Carlo cross-validation ("Subsample"),
* Holdout (training/test) ("Holdout").

For example if you want to use 3-fold cross-validation type:
```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)
rdesc
```

For holdout estimation use:
```{r}
## Holdout estimation
rdesc = makeResampleDesc("Holdout")
rdesc
```

In order to save you some typing mlr contains some pre-defined resample descriptions for very common strategies like holdout (hout) as well as cross-validation with different numbers of folds (e.g., cv5 or cv10).

```{r}
hout
```

```{r}
cv3
```

## Resampling
Function `resample` evaluates a Learner on a given machine learning Task using the selected resampling strategy.

As usual, you can either pass a Learner object to resample or, as done here, provide the class name "regr.lm" of the learner. Since no performance measure is specified the default for regression learners (mean squared error, mse) is calculated.

```{r}
## Specify the resampling strategy (3-fold cross-validation)
rdesc = makeResampleDesc("CV", iters = 3)

## Calculate the performance
r = resample("regr.lm", bh.task, rdesc)

r
```

The result r is an object of class ResampleResult. It contains performance results for the learner and some additional information like the runtime, predicted values, and optionally the models fitted in single resampling iterations.

```{r}
## Peak into r
names(r)
```

```{r}
r$aggr
```

```{r}
r$measures.test
```
r$measures.test gives the performance on each of the 3 test data sets. r$aggr shows the aggregated performance value. Its name "mse.test.mean" indicates the performance measure, mse, and the method, test.mean, used to aggregate the 3 individual performances. test.mean is the default aggregation scheme for most performance measures and, as the name implies, takes the mean over the performances on the test data sets.

Now a a classification example with several measures at once by passing a list of Measures to resample. 
In each subsampling iteration the data set is randomly partitioned into a training and a test set according to a given percentage, e.g., 2/3 training and 1/3 test set. If there is just one iteration, the strategy is commonly called holdout or test sample estimation.

```{r}
## Subsampling with 5 iterations and default split ratio 2/3
rdesc = makeResampleDesc("Subsample", iters = 5)

## Subsampling with 5 iterations and 4/5 training data
rdesc = makeResampleDesc("Subsample", iters = 5, split = 4/5)

## Classification tree with information splitting criterion
lrn = makeLearner("classif.rpart", parms = list(split = "information"))

## Calculate the performance measures
r = resample(lrn, sonar.task, rdesc, measures = list(mmce, fpr, fnr, timetrain))

r
```

If you want to add further measures afterwards, use addRRMeasure.
```{r}
## Add balanced error rate (ber) and time used to predict
addRRMeasure(r, list(ber, timepredict))
```

By default, resample prints progress messages and intermediate results. You can turn this off by setting show.info = FALSE, as done in the code chunk below. 

In the above example, the Learner was explicitly constructed. For convenience you can also specify the learner as a string and pass any learner parameters via the ... argument of resample.

```{r}
r = resample("classif.rpart", 
             parms = list(split = "information"), 
             sonar.task, 
             rdesc, 
             measures = list(mmce, fpr, fnr, timetrain),
             show.info = FALSE)

r
```

## Accessing resample results
### Predictions
Per default, the ResampleResult contains the predictions made during the resampling. If you do not want to keep them, e.g., in order to conserve memory, set keep.pred = FALSE when calling resample.

The predictions are stored in slot $pred of the resampling result, which can also be accessed by function getRRPredictions.
```{r}
r$pred
```

```{r}
pred = getRRPredictions(r)
pred
```

pred is an object of class ResamplePrediction. Just as a Prediction object it has an element `$data` which is a data.frame that contains the predictions and in the case of a supervised learning problem the true values of the target variable(s). You can use as.data.frame to directly access the `$data` slot. Moreover, all getter functions for Prediction objects like getPredictionResponse or getPredictionProbabilities are applicable.

```{r}
head(as.data.frame(pred))
```

```{r}
head(getPredictionTruth(pred))
```

```{r}
head(getPredictionResponse(pred))
```

The columns iter and set in the data.frame indicate the resampling iteration and the data set (train or test) for which the prediction was made.

By default, predictions are made for the test sets only. If predictions for the training set are required, set predict = "train" (for predictions on the train set only) or predict = "both" (for predictions on both train and test sets) in makeResampleDesc. In any case, this is necessary for some bootstrap methods (b632 and b632+) and some examples are shown later on.

Below, we use simple Holdout, i.e., split the data once into a training and test set, as resampling strategy and make predictions on both sets.

```{r}
## Make predictions on both training and test sets
rdesc = makeResampleDesc("Holdout", predict = "both")

r = resample("classif.lda", iris.task, rdesc, show.info = FALSE)
r
```

```{r}
r$measures.train
```

(Please note that nonetheless the misclassification rate r$aggr is estimated on the test data only. How to calculate performance measures on the training sets is shown below.)

A second function to extract predictions from resample results is getRRPredictionList which returns a list of predictions split by data set (train/test) and resampling iteration.
```{r}
predList = getRRPredictionList(r)
predList
```
### Learner models
In each resampling iteration a Learner is fitted on the respective training set. By default, the resulting WrappedModels are not included in the ResampleResult and slot $models is empty. In order to keep them, set models = TRUE when calling resample, as in the following survival analysis example.

```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)

r = resample("surv.coxph", lung.task, rdesc, show.info = FALSE, models = TRUE)
r$models
```
### The extract option
Keeping complete fitted models can be memory-intensive if these objects are large or the number of resampling iterations is high. Alternatively, you can use the extract argument of resample to retain only the information you need. To this end you need to pass a function to extract which is applied to each WrappedModel object fitted in each resampling iteration.

Below, we cluster the mtcars data using the kk-means algorithm with k=3k=3 and keep only the cluster centers.
```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)

## Extract the compute cluster centers
r = resample("cluster.kmeans", mtcars.task, rdesc, show.info = FALSE,
             centers = 3, 
             extract = function(x) getLearnerModel(x)$centers)
r$extract
```
As a second example, we extract the variable importances from fitted regression trees using function getFeatureImportance.
```{r}
## Extract the variable importance in a regression tree
r = resample("regr.rpart", bh.task, rdesc, show.info = FALSE, 
             extract = getFeatureImportance)
r$extract
```
## Stratification and blocking

* Stratification with respect to a categorical variable makes sure that all its values are present in each training and test set in approximately the same proportion as in the original data set. Stratification is possible with regard to categorical target variables (and thus for supervised classification and survival analysis) or categorical explanatory variables.
* Blocking refers to the situation that subsets of observations belong together and must not be separated during resampling. Hence, for one train/test set pair the entire block is either in the training set or in the test set.

### Stratification with respect to the target variable(s)
For classification, it is usually desirable to have the same proportion of the classes in all of the partitions of the original data set. This is particularly useful in the case of imbalanced classes and small data sets. Otherwise, it may happen that observations of less frequent classes are missing in some of the training sets which can decrease the performance of the learner, or lead to model crashes. In order to conduct stratified resampling, set stratify = TRUE in makeResampleDesc.
```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3, stratify = TRUE)

r = resample("classif.lda", iris.task, rdesc, show.info = FALSE)
r
```

Stratification is also available for survival tasks. Here the stratification balances the censoring rate.

### Stratification with respect to explanatory variables
Sometimes it is required to also stratify on the input data, e.g., to ensure that all subgroups are represented in all training and test sets. To stratify on the input columns, specify factor columns of your task data via stratify.cols.
```{r}
rdesc = makeResampleDesc("CV", iters = 3, stratify.cols = "chas")

r = resample("regr.rpart", bh.task, rdesc, show.info = FALSE)
r
```

### Blocking
If some observations "belong together" and must not be separated when splitting the data into training and test sets for resampling, you can supply this information via a blocking factor when creating the task.

```{r}
## 5 blocks containing 30 observations each
task = makeClassifTask(data = iris, target = "Species", blocking = factor(rep(1:5, each = 30)))
task
```

## Resample descriptions and resample instances
As already mentioned, you can specify a resampling strategy using function makeResampleDesc.
```{r}
rdesc = makeResampleDesc("CV", iters = 3)
rdesc
```

```{r}
str(rdesc)
```

```{r}
str(makeResampleDesc("Subsample", stratify.cols = "chas"))
```

Given either the size of the data set at hand or the Task, function makeResampleInstance draws the training and test sets according to the ResampleDesc.

```{r}
## Create a resample instance based an a task
rin = makeResampleInstance(rdesc, iris.task)
rin
```

```{r}
str(rin)
```

```{r}
## Create a resample instance given the size of the data set
rin = makeResampleInstance(rdesc, size = nrow(iris))
str(rin)
```

```{r}
## Access the indices of the training observations in iteration 3
rin$train.inds[[3]]
```

If a ResampleDesc is passed to resample, it is instantiated internally. Naturally, it is also possible to pass a ResampleInstance directly.

While the separation between resample descriptions, resample instances, and the resample function itself seems overly complicated, it has several advantages:

* Resample instances readily allow for paired experiments, that is comparing the performance of several learners on exactly the same training and test sets. This is particularly useful if you want to add another method to a comparison experiment you already did. Moreover, you can store the resample instance along with your data in order to be able to reproduce your results later on.

```{r}
rdesc = makeResampleDesc("CV", iters = 3)
rin = makeResampleInstance(rdesc, task = iris.task)

## Calculate the performance of two learners based on the same resample instance
r.lda = resample("classif.lda", iris.task, rin, show.info = FALSE)
r.rpart = resample("classif.rpart", iris.task, rin, show.info = FALSE)
r.lda$aggr
```

```{r}
r.rpart$aggr
```

* In order to add further resampling methods you can simply derive from the ResampleDesc and ResampleInstance classes, but you do neither have to touch resample nor any further methods that use the resampling strategy.

Usually, when calling makeResampleInstance the train and test index sets are drawn randomly. Mainly for holdout (test sample) estimation you might want full control about the training and tests set and specify them manually. This can be done using function makeFixedHoldoutInstance.

```{r}
rin = makeFixedHoldoutInstance(train.inds = 1:100, 
                               test.inds = 101:150, size = 150)
rin
```

## Aggregating performance values
For the great majority of common resampling strategies (like holdout, cross-validation, subsampling) performance values are calculated on the test data sets only and for most measures aggregated by taking the mean (test.mean).

Each performance Measure in mlr has a corresponding default aggregation method which is stored in slot $aggr. The default aggregation for most measures is test.mean. One exception is the root mean square error (rmse).

```{r}
## Mean misclassification error
mmce$aggr
```

```{r}
mmce$aggr$fun
```

```{r}
## Root mean square error
rmse$aggr
```

```{r}
rmse$aggr$fun
```

You can change the aggregation method of a Measure via function setAggregation. All available aggregation schemes are listed on the aggregations documentation page.

### Example: One measure with different aggregations
The aggregation schemes test.median, test.min, and test.max compute the median, minimum, and maximum of the performance values on the test sets.

```{r}
mseTestMedian = setAggregation(mse, test.median)
mseTestMin = setAggregation(mse, test.min)
mseTestMax = setAggregation(mse, test.max)

mseTestMedian
```

```{r}
rdesc = makeResampleDesc("CV", iters = 3)
r = resample("regr.lm", bh.task, rdesc, 
             measures = list(mse, mseTestMedian, mseTestMin, mseTestMax))
r
```


```{r}
r$aggr
```

### Example: Bootstrap

The b632 and b632+ variants of out-of-bag bootstrap estimation calculate a convex combination of the training performance and the out-of-bag bootstrap performance and thus require predictions on the training sets and an appropriate aggregation strategy.

```{r}
## Use bootstrap as resampling strategy and predict on both train and test sets
rdesc = makeResampleDesc("Bootstrap", predict = "both", iters = 10)

## Set aggregation schemes for b632 and b632+ bootstrap
mmceB632 = setAggregation(mmce, b632)
mmceB632plus = setAggregation(mmce, b632plus)

mmceB632
```

```{r}
r = resample("classif.rpart", iris.task, rdesc, 
             measures = list(mmce, mmceB632, mmceB632plus),
             show.info = FALSE)
head(r$measures.train)
```

```{r}
## Compare misclassification rates for out-of-bag, b632, and b632+ bootstrap
r$aggr
```

## Convenience functions
The functionality described on this page allows for much control and flexibility. However, when quickly trying out some learners, it can get tedious to type all the code for defining the resampling strategy, setting the aggregation scheme and so on. As mentioned above, mlr includes some pre-defined resample description objects for frequently used strategies like, e.g., 5-fold cross-validation (cv5). Moreover, mlr provides special functions for the most common resampling methods, for example holdout, crossval, or bootstrapB632.

```{r}
crossval("classif.lda", iris.task, iters = 3, measures = list(mmce, ber))
```

```{r}
bootstrapB632plus("regr.lm", bh.task, iters = 3, measures = list(mse, mae))
```

